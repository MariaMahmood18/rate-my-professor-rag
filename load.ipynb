{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MiniConda\\envs\\rag\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'rag' created successfully.\n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Emily Smith: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. John Doe: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Sarah Johnson: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. Michael Brown: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Linda Davis: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. James Wilson: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Karen Taylor: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. Robert Martinez: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Nancy Anderson: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. Charles Thomas: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Sandra Harris: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. Paul White: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Laura Moore: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. Mark Jackson: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Barbara Clark: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. Steven Lewis: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Jessica Lee: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. George Walker: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Dr. Patricia Young: GeminiAI API request failed: 404 - \n",
      "Response status code: 404\n",
      "Response body: \n",
      "Error processing review for professor Prof. Thomas Hall: GeminiAI API request failed: 404 - \n",
      "No embeddings were generated, skipping the upsert operation.\n",
      "Error describing index stats: name 'index' is not defined\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import schema\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Load environment variables from .env.local\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# Define the index name and dimension\n",
    "index_name = \"rag\"\n",
    "dimension = 1536\n",
    "\n",
    "# Check if the index exists before creating it\n",
    "existing_indexes = pc.list_indexes()\n",
    "if index_name not in existing_indexes:\n",
    "    try:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        )\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating index: {e}\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Initialize GeminiAI API client\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/gemini-1_5-flash:embedText\"  # Updated API URL\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Function to get embedding from the GeminiAI API.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GEMINI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gemini-1.5-flash\",\n",
    "        \"text\": text\n",
    "    }\n",
    "    response = requests.post(GEMINI_API_URL, json=payload, headers=headers)\n",
    "    print(f\"Response status code: {response.status_code}\")\n",
    "    print(f\"Response body: {response.text}\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"GeminiAI API request failed: {response.status_code} - {response.text}\")\n",
    "    return response.json().get(\"embedding\")\n",
    "\n",
    "# Load the review data\n",
    "with open(\"reviews.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "# Create embeddings for each review\n",
    "for review in data.get(\"reviews\", []):\n",
    "    try:\n",
    "        embedding = get_embedding(review['review'])\n",
    "        if embedding:\n",
    "            processed_data.append(\n",
    "                {\n",
    "                    \"values\": embedding,\n",
    "                    \"id\": review[\"professor\"],\n",
    "                    \"metadata\": {\n",
    "                        \"review\": review[\"review\"],\n",
    "                        \"subject\": review[\"subject\"],\n",
    "                        \"stars\": review[\"stars\"],\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(f\"No embedding returned for review: {review['review']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing review for professor {review.get('professor', 'unknown')}: {e}\")\n",
    "\n",
    "# Check if there are embeddings to upsert\n",
    "if processed_data:\n",
    "    # Insert the embeddings into the Pinecone index\n",
    "    index = pc.Index(index_name)\n",
    "    try:\n",
    "        upsert_response = index.upsert(\n",
    "            vectors=processed_data,\n",
    "            namespace=\"ns1\",\n",
    "        )\n",
    "        print(f\"Upserted count: {upsert_response['upserted_count']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting vectors into Pinecone: {e}\")\n",
    "else:\n",
    "    print(\"No embeddings were generated, skipping the upsert operation.\")\n",
    "\n",
    "# Print index statistics\n",
    "try:\n",
    "    stats = index.describe_index_stats()\n",
    "    print(\"Index statistics:\", stats)\n",
    "except Exception as e:\n",
    "    print(f\"Error describing index stats: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received in 3.17 seconds\n",
      "Response received in 1.22 seconds\n",
      "Response received in 2.66 seconds\n",
      "Response received in 3.69 seconds\n",
      "Response received in 1.44 seconds\n",
      "Response received in 0.81 seconds\n",
      "Response received in 2.56 seconds\n",
      "Response received in 2.87 seconds\n",
      "Response received in 1.43 seconds\n",
      "Response received in 3.58 seconds\n",
      "Response received in 2.36 seconds\n",
      "Response received in 2.97 seconds\n",
      "Response received in 1.02 seconds\n",
      "Response received in 2.35 seconds\n",
      "Response received in 2.76 seconds\n",
      "Response received in 1.54 seconds\n",
      "Response received in 1.64 seconds\n",
      "Response received in 2.55 seconds\n",
      "Response received in 2.15 seconds\n",
      "Response received in 2.25 seconds\n",
      "Error upserting vectors into Pinecone: Expected a list or list-like data structure, but got: This is a positive and concise review of a lecturer!  It captures the key aspects of a good instructor:\n",
      "\n",
      "* **Clarity:**  \"Great lecturer with clear explanations\" emphasizes the instructor's ability to communicate effectively.\n",
      "* **Challenge:** \"Assignments were challenging but fair\" acknowledges the intellectual rigor of the course while assuring that the demands were reasonable. \n",
      "\n",
      "You could expand on this review to be even more detailed and helpful for others:\n",
      "\n",
      "* **What specific topics were explained clearly?**  \n",
      "* **What type of assignments were challenging?** (e.g., problem sets, essays, projects)\n",
      "* **What made the assignments fair?** (e.g., adequate time to complete, clear expectations, helpful feedback)\n",
      "* **What specific aspects of the lecturer's teaching style made them great?** (e.g., enthusiasm, humor, interactive teaching methods) \n",
      "\n",
      "By adding these details, your review will be even more valuable for students considering this lecturer's class. \n",
      "\n",
      "Index statistics: {'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Load environment variables from .env.local\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# Access environment variables\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if api_key is None or pinecone_api_key is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY or PINECONE_API_KEY environment variables are not set.\")\n",
    "\n",
    "# Configure the GeminiAI API client\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Initialize the GeminiAI model\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Define the index name\n",
    "index_name = \"rag\"\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Function to get embedding from the GeminiAI API.\"\"\"\n",
    "    try:\n",
    "        response = model.generate_content(text)\n",
    "        print(f\"Response received in {time.time() - start_time:.2f} seconds\")\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the review data\n",
    "with open(\"reviews.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "# Create embeddings for each review\n",
    "for review in data.get(\"reviews\", []):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        embedding = get_embedding(review['review'])\n",
    "        if embedding:\n",
    "            processed_data.append(\n",
    "                {\n",
    "                    \"values\": embedding,\n",
    "                    \"id\": review[\"professor\"],\n",
    "                    \"metadata\": {\n",
    "                        \"review\": review[\"review\"],\n",
    "                        \"subject\": review[\"subject\"],\n",
    "                        \"stars\": review[\"stars\"],\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(f\"No embedding returned for review: {review['review']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing review for professor {review.get('professor', 'unknown')}: {e}\")\n",
    "\n",
    "# Check if there are embeddings to upsert\n",
    "if processed_data:\n",
    "    # Insert the embeddings into the Pinecone index\n",
    "    index = pc.Index(index_name)\n",
    "    try:\n",
    "        upsert_response = index.upsert(\n",
    "            vectors=processed_data,\n",
    "            namespace=\"ns1\",\n",
    "        )\n",
    "        print(f\"Upserted count: {upsert_response['upserted_count']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting vectors into Pinecone: {e}\")\n",
    "else:\n",
    "    print(\"No embeddings were generated, skipping the upsert operation.\")\n",
    "\n",
    "# Print index statistics\n",
    "try:\n",
    "    stats = index.describe_index_stats()\n",
    "    print(\"Index statistics:\", stats)\n",
    "except Exception as e:\n",
    "    print(f\"Error describing index stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received in 8.53 seconds\n",
      "## Unpacking the Magic Box: How an LLM Works\n",
      "\n",
      "Let's break down how these powerful language models (LLMs) actually function. Here's a simplified explanation:\n",
      "\n",
      "**1. The Core:  A Deep Neural Network**\n",
      "\n",
      "At the heart of an LLM lies a deep neural network (DNN). Imagine it as a complex web of interconnected nodes, organized in multiple layers. These nodes are like tiny processors, performing mathematical calculations based on the data they receive.\n",
      "\n",
      "**2. The Input: Feeding the Model**\n",
      "\n",
      "You feed an LLM text data – think massive amounts of books, articles, code, and more. This data is processed and converted into numerical representations (vectors) that the model can understand. \n",
      "\n",
      "**3. Learning Through Prediction**\n",
      "\n",
      "The model then learns by predicting the next word in a sequence. It analyzes the input text, looks for patterns and relationships between words, and uses this information to guess the most likely next word. \n",
      "\n",
      "**4. The Magic of Transformers**\n",
      "\n",
      "Many modern LLMs use a technique called \"Transformers\" which revolutionized how these models process language. Instead of analyzing text sequentially, Transformers consider all words in a sentence simultaneously. This enables them to understand complex relationships between words and capture context more effectively.\n",
      "\n",
      "**5. Adjusting the Weights**\n",
      "\n",
      "During training, the model constantly adjusts the connections between nodes (called \"weights\").  These weights determine how strongly one node influences another, and they're fine-tuned to improve the model's predictive accuracy.\n",
      "\n",
      "**6. Generating Text:**  The Language Maker\n",
      "\n",
      "Once trained, an LLM can generate coherent text based on a prompt. You give it a starting phrase or context, and it uses its learned knowledge to continue the text, creating new, plausible sentences that fit the provided information.\n",
      "\n",
      "**7. Fine-Tuning for Specific Tasks**\n",
      "\n",
      "You can further refine an LLM for specific tasks like translation, question answering, or code generation. This involves providing the model with additional training data and adjusting its parameters to optimize its performance for that specific purpose.\n",
      "\n",
      "**In a nutshell:**\n",
      "\n",
      "* **LLMs learn by predicting the next word in a sequence.**\n",
      "* **Transformers help LLMs understand complex relationships between words.**\n",
      "* **Trained LLMs can generate text, answer questions, translate languages, and perform various other language-based tasks.**\n",
      "\n",
      "**Important to Note:**\n",
      "\n",
      "* LLMs are still under development, and ongoing research aims to improve their accuracy, efficiency, and ability to understand nuances of language.\n",
      "* These models are powerful, but they are not sentient. They are sophisticated algorithms that mimic human language abilities but lack true understanding or consciousness.\n",
      "\n",
      "Let me know if you'd like to explore any specific aspect of LLM technology in more detail! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Access environment variable correctly\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if api_key is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable is not set.\")\n",
    "\n",
    "# Configure the API client\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "\n",
    "# Generate content\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = model.generate_content('Teach me about how an LLM works')\n",
    "    end_time = time.time()\n",
    "    print(f\"Response received in {end_time - start_time:.2f} seconds\")\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
